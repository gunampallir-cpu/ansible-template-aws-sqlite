name: AWS CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      action:
        description: 'Action to perform'
        required: true
        default: 'deploy'
        type: choice
        options:
          - deploy
          - destroy

env:
  AWS_REGION: us-east-1
  EKS_CLUSTER_NAME: ansible-template-eks
  BACKEND_ECR_REPO: backend
  FRONTEND_ECR_REPO: frontend

permissions:
  contents: read
  security-events: write
  actions: read

jobs:
  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: |
            backend/package-lock.json
            frontend/package-lock.json

      - name: Install backend dependencies
        working-directory: ./backend
        run: npm install

      - name: Install frontend dependencies
        working-directory: ./frontend
        run: npm install

      - name: Lint backend
        working-directory: ./backend
        run: npm run lint || true

      - name: Lint frontend
        working-directory: ./frontend
        run: npm run lint

      - name: Run backend tests
        working-directory: ./backend
        run: npm test || echo "No tests configured"

      - name: Run frontend tests
        working-directory: ./frontend
        run: npm test -- --passWithNoTests

  security-scan:
    name: Security Scanning
    runs-on: ubuntu-latest
    needs: code-quality
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run Trivy vulnerability scanner (filesystem)
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-fs-results.sarif'
          severity: 'CRITICAL,HIGH'

      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-fs-results.sarif'

  build-backend:
    name: Build Backend Image
    runs-on: ubuntu-latest
    needs: security-scan
    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
      image-digest: ${{ steps.build.outputs.digest }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Create ECR repository if not exists
        run: |
          aws ecr describe-repositories --repository-names ${{ env.BACKEND_ECR_REPO }} --region ${{ env.AWS_REGION }} || \
          aws ecr create-repository --repository-name ${{ env.BACKEND_ECR_REPO }} --region ${{ env.AWS_REGION }} \
            --image-scanning-configuration scanOnPush=true \
            --encryption-configuration encryptionType=AES256

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Extract metadata for Docker
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ steps.login-ecr.outputs.registry }}/${{ env.BACKEND_ECR_REPO }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push backend image
        id: build
        uses: docker/build-push-action@v5
        with:
          context: ./backend
          file: ./backend/Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  scan-backend:
    name: Scan Backend Image
    runs-on: ubuntu-latest
    needs: build-backend
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Run Trivy vulnerability scanner (backend image)
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'image'
          image-ref: '${{ steps.login-ecr.outputs.registry }}/${{ env.BACKEND_ECR_REPO }}:latest'
          format: 'sarif'
          output: 'trivy-backend-results.sarif'
          severity: 'CRITICAL,HIGH'

      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-backend-results.sarif'

  build-frontend:
    name: Build Frontend Image
    runs-on: ubuntu-latest
    needs: security-scan
    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
      image-digest: ${{ steps.build.outputs.digest }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Create ECR repository if not exists
        run: |
          aws ecr describe-repositories --repository-names ${{ env.FRONTEND_ECR_REPO }} --region ${{ env.AWS_REGION }} || \
          aws ecr create-repository --repository-name ${{ env.FRONTEND_ECR_REPO }} --region ${{ env.AWS_REGION }} \
            --image-scanning-configuration scanOnPush=true \
            --encryption-configuration encryptionType=AES256

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Extract metadata for Docker
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ steps.login-ecr.outputs.registry }}/${{ env.FRONTEND_ECR_REPO }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push frontend image
        id: build
        uses: docker/build-push-action@v5
        with:
          context: ./frontend
          file: ./frontend/Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  scan-frontend:
    name: Scan Frontend Image
    runs-on: ubuntu-latest
    needs: build-frontend
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Run Trivy vulnerability scanner (frontend image)
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'image'
          image-ref: '${{ steps.login-ecr.outputs.registry }}/${{ env.FRONTEND_ECR_REPO }}:latest'
          format: 'sarif'
          output: 'trivy-frontend-results.sarif'
          severity: 'CRITICAL,HIGH'

      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-frontend-results.sarif'

  terraform-plan:
    name: Terraform Plan
    runs-on: ubuntu-latest
    needs: [scan-backend, scan-frontend]
    defaults:
      run:
        working-directory: ./terraform-aws
    outputs:
      tfplan-exitcode: ${{ steps.plan.outputs.exitcode }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0

      - name: Create Terraform Backend Resources
        run: |
          # Create S3 bucket for Terraform state if it doesn't exist
          if ! aws s3api head-bucket --bucket ansible-template-tfstate 2>/dev/null; then
            echo "Creating S3 bucket for Terraform state..."
            aws s3api create-bucket \
              --bucket ansible-template-tfstate \
              --region us-east-1
            
            # Enable versioning
            aws s3api put-bucket-versioning \
              --bucket ansible-template-tfstate \
              --versioning-configuration Status=Enabled
            
            # Enable encryption
            aws s3api put-bucket-encryption \
              --bucket ansible-template-tfstate \
              --server-side-encryption-configuration '{
                "Rules": [{
                  "ApplyServerSideEncryptionByDefault": {
                    "SSEAlgorithm": "AES256"
                  }
                }]
              }'
            
            # Block public access
            aws s3api put-public-access-block \
              --bucket ansible-template-tfstate \
              --public-access-block-configuration \
                BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true
            
            echo "S3 bucket created successfully"
          else
            echo "S3 bucket already exists"
          fi

      - name: Terraform Init
        run: terraform init

      - name: Import Existing Resources
        run: |
          # Import existing resources if they exist (ignore errors if already in state)
          
          # Import OIDC Provider
          OIDC_URL=$(aws eks describe-cluster --name ansible-template-eks --query 'cluster.identity.oidc.issuer' --output text 2>/dev/null | sed 's|https://||' || echo "")
          if [ ! -z "$OIDC_URL" ]; then
            OIDC_ARN="arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):oidc-provider/${OIDC_URL}"
            terraform import aws_iam_openid_connect_provider.eks "$OIDC_ARN" 2>/dev/null || echo "OIDC provider already in state or doesn't exist"
          fi
          
          # Import Node Group
          terraform import aws_eks_node_group.main "ansible-template-eks:ansible-template-node-group" 2>/dev/null || echo "Node group already in state or doesn't exist"
          
          # Import Load Balancer Controller IAM Policy
          terraform import aws_iam_policy.aws_load_balancer_controller "arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):policy/AWSLoadBalancerControllerIAMPolicy" 2>/dev/null || echo "IAM policy already in state or doesn't exist"
          
          # Import Load Balancer Controller Role
          terraform import aws_iam_role.aws_load_balancer_controller "ansible-template-aws-load-balancer-controller" 2>/dev/null || echo "IAM role already in state or doesn't exist"
          
          # Import Load Balancer Controller Role Policy Attachment
          terraform import aws_iam_role_policy_attachment.aws_load_balancer_controller "ansible-template-aws-load-balancer-controller/arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):policy/AWSLoadBalancerControllerIAMPolicy" 2>/dev/null || echo "Policy attachment already in state or doesn't exist"
          
          # Import EKS Addons (only vpc-cni and kube-proxy, skip coredns and ebs-csi-driver)
          terraform import aws_eks_addon.vpc_cni "ansible-template-eks:vpc-cni" 2>/dev/null || echo "vpc-cni addon already in state or doesn't exist"
          terraform import aws_eks_addon.kube_proxy "ansible-template-eks:kube-proxy" 2>/dev/null || echo "kube-proxy addon already in state or doesn't exist"
        continue-on-error: true

      - name: Terraform Format Check
        run: terraform fmt -check || true

      - name: Terraform Validate
        run: terraform validate

      - name: Terraform Plan
        id: plan
        run: |
          terraform plan -detailed-exitcode -no-color -out=tfplan || echo "exitcode=$?" >> $GITHUB_OUTPUT
        continue-on-error: true

      - name: Upload Terraform Plan
        uses: actions/upload-artifact@v4
        with:
          name: tfplan
          path: terraform-aws/tfplan
          retention-days: 5

      - name: Comment PR with Terraform Plan
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const output = `#### Terraform Plan ðŸ“–
            - Format Check: \`${{ steps.plan.outcome }}\`
            - Validation: âœ…
            - Plan: \`${{ steps.plan.outcome }}\`
            
            <details><summary>Show Plan</summary>
            
            \`\`\`terraform
            ${{ steps.plan.outputs.stdout }}
            \`\`\`
            
            </details>
            
            *Pusher: @${{ github.actor }}, Action: \`${{ github.event_name }}\`, Workflow: \`${{ github.workflow }}\`*`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: output
            })

  terraform-apply:
    name: Terraform Apply
    runs-on: ubuntu-latest
    needs: terraform-plan
    if: github.ref == 'refs/heads/main' && github.event_name == 'push' && github.event.inputs.action != 'destroy'
    defaults:
      run:
        working-directory: ./terraform-aws
    environment:
      name: production
      url: https://ansible-template.example.com
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0

      - name: Create Terraform Backend Resources
        run: |
          # Create S3 bucket for Terraform state if it doesn't exist
          if ! aws s3api head-bucket --bucket ansible-template-tfstate 2>/dev/null; then
            echo "Creating S3 bucket for Terraform state..."
            aws s3api create-bucket \
              --bucket ansible-template-tfstate \
              --region us-east-1
            
            # Enable versioning
            aws s3api put-bucket-versioning \
              --bucket ansible-template-tfstate \
              --versioning-configuration Status=Enabled
            
            # Enable encryption
            aws s3api put-bucket-encryption \
              --bucket ansible-template-tfstate \
              --server-side-encryption-configuration '{
                "Rules": [{
                  "ApplyServerSideEncryptionByDefault": {
                    "SSEAlgorithm": "AES256"
                  }
                }]
              }'
            
            # Block public access
            aws s3api put-public-access-block \
              --bucket ansible-template-tfstate \
              --public-access-block-configuration \
                BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true
            
            echo "S3 bucket created successfully"
          else
            echo "S3 bucket already exists"
          fi

      - name: Terraform Init
        run: terraform init

      - name: Download Terraform Plan
        uses: actions/download-artifact@v4
        with:
          name: tfplan
          path: terraform-aws/

      - name: Terraform Apply
        run: terraform apply -auto-approve tfplan

  deploy-to-eks:
    name: Deploy to EKS
    runs-on: ubuntu-latest
    needs: [terraform-apply, build-backend, build-frontend]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    environment:
      name: production
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/v1.28.0/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/
          kubectl version --client

      - name: Install Helm
        run: |
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          helm version

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}

      - name: Verify cluster access and diagnose node issues
        run: |
          echo "=== EKS Cluster Information ==="
          aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} --query 'cluster.{Name:name,Status:status,Version:version,Endpoint:endpoint}' --output table
          
          echo ""
          echo "=== Node Groups ==="
          aws eks list-nodegroups --cluster-name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }}
          
          echo ""
          echo "=== Node Group Details ==="
          for nodegroup in $(aws eks list-nodegroups --cluster-name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} --query 'nodegroups[]' --output text); do
            echo "Node Group: $nodegroup"
            aws eks describe-nodegroup --cluster-name ${{ env.EKS_CLUSTER_NAME }} --nodegroup-name $nodegroup --region ${{ env.AWS_REGION }} --query 'nodegroup.{Status:status,DesiredSize:scalingConfig.desiredSize,MinSize:scalingConfig.minSize,MaxSize:scalingConfig.maxSize,Health:health}' --output table
          done
          
          echo ""
          echo "=== Kubernetes Nodes ==="
          kubectl get nodes -o wide || echo "âš ï¸ NO NODES FOUND IN CLUSTER"
          
          echo ""
          echo "=== Node Group EC2 Instances ==="
          aws ec2 describe-instances --region ${{ env.AWS_REGION }} \
            --filters "Name=tag:eks:cluster-name,Values=${{ env.EKS_CLUSTER_NAME }}" \
            --query 'Reservations[].Instances[].[InstanceId,State.Name,InstanceType,PrivateIpAddress,LaunchTime]' \
            --output table || echo "No EC2 instances found"
          
          echo ""
          echo "=== Namespaces ==="
          kubectl get namespaces
        continue-on-error: true
          
      - name: Check cluster health
        run: |
          echo "Checking system pods..."
          kubectl get pods -n kube-system -o wide || echo "No kube-system pods"
          
          echo "Checking CoreDNS status..."
          kubectl get pods -n kube-system -l k8s-app=kube-dns || echo "No CoreDNS pods"
          
          echo "Checking for any issues..."
          kubectl get events -n kube-system --sort-by='.lastTimestamp' | tail -20 || echo "No events found"
        continue-on-error: true

      - name: Deploy backend with Helm
        run: |
          helm upgrade --install ansible-template-backend ./helm/backend \
            --namespace production \
            --create-namespace \
            --set image.tag=latest \
            --wait \
            --timeout 5m \
            --debug
        continue-on-error: true

      - name: Check backend deployment status
        if: always()
        run: |
          echo "=== Backend Pods ==="
          kubectl get pods -n production -l app.kubernetes.io/name=ansible-template-backend -o wide
          
          echo ""
          echo "=== Pod Events (to see why pods are Pending) ==="
          kubectl get events -n production --field-selector involvedObject.kind=Pod --sort-by='.lastTimestamp' | tail -30
          
          echo ""
          echo "=== Describe Pending Pods ==="
          for pod in $(kubectl get pods -n production -l app.kubernetes.io/name=ansible-template-backend -o jsonpath='{.items[?(@.status.phase=="Pending")].metadata.name}'); do
            echo "Pod: $pod"
            kubectl describe pod $pod -n production
          done || echo "No pending backend pods or describe failed"
          
          echo ""
          echo "=== Pod Logs (if any) ==="
          kubectl logs -n production -l app.kubernetes.io/name=ansible-template-backend --tail=50 --all-containers=true || echo "No logs available"
        continue-on-error: true

      - name: Deploy frontend with Helm
        run: |
          helm upgrade --install ansible-template-frontend ./helm/frontend \
            --namespace production \
            --create-namespace \
            --set image.tag=latest \
            --wait \
            --timeout 5m \
            --debug
        continue-on-error: true

      - name: Check frontend deployment status
        if: always()
        run: |
          echo "=== Frontend Pods ==="
          kubectl get pods -n production -l app.kubernetes.io/name=ansible-template-frontend -o wide
          
          echo ""
          echo "=== Describe Pending Pods ==="
          for pod in $(kubectl get pods -n production -l app.kubernetes.io/name=ansible-template-frontend -o jsonpath='{.items[?(@.status.phase=="Pending")].metadata.name}'); do
            echo "Pod: $pod"
            kubectl describe pod $pod -n production
          done || echo "No pending frontend pods or describe failed"
          
          echo ""
          echo "=== Pod Logs (if any) ==="
          kubectl logs -n production -l app.kubernetes.io/name=ansible-template-frontend --tail=50 --all-containers=true || echo "No logs available"
        continue-on-error: true

      - name: Verify deployment
        run: |
          kubectl get pods -n production
          kubectl get services -n production
          kubectl get ingress -n production || echo "No ingress found"

      - name: Get application endpoints
        run: |
          echo "Getting service endpoints..."
          kubectl get svc -n production -o wide
          
          # Check if Istio is installed
          if kubectl get namespace istio-ingress 2>/dev/null; then
            echo "Getting Istio Ingress Gateway URL..."
            kubectl get svc -n istio-ingress istio-ingress -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' || echo "Istio gateway not ready"
          else
            echo "Istio is not installed, checking for LoadBalancer services..."
            kubectl get svc -n production -o json | jq -r '.items[] | select(.spec.type=="LoadBalancer") | "\(.metadata.name): \(.status.loadBalancer.ingress[0].hostname // .status.loadBalancer.ingress[0].ip // "pending")"' || echo "No LoadBalancer services found"
          fi
        continue-on-error: true

  destroy-infrastructure:
    name: Destroy Infrastructure
    runs-on: ubuntu-latest
    if: github.event.inputs.action == 'destroy'
    defaults:
      run:
        working-directory: ./terraform-aws
    environment:
      name: production
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/v1.28.0/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/
          kubectl version --client
        continue-on-error: true

      - name: Install Helm
        run: |
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          helm version
        continue-on-error: true

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}
        continue-on-error: true

      - name: Delete Helm releases
        run: |
          helm uninstall ansible-template-backend -n production || true
          helm uninstall ansible-template-frontend -n production || true
        continue-on-error: true

      - name: Delete namespace
        run: |
          kubectl delete namespace production --timeout=5m || true
        continue-on-error: true

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0

      - name: Terraform Init
        run: terraform init

      - name: Terraform Destroy
        run: terraform destroy -auto-approve
